<!-- Generated by SpecStory -->

# Troubleshooting ComfyUI CUDA error (2025-07-15 10:40Z)

_**User**_

this is my device config:\
  Instance ID: 23019700                                
                                                       
                │
  │   Host: 79466                                      
                                                       
                    │
  │   Machine ID: 13047                                
                                                       
                    │
  │   Quebec, CA                                       
                                                       
                    │
  │   Max CUDA: 12.4                                   
                                                       
                    │
  │   15.7 TFLOPS                                      
                                                       
                    │
  │   VRAM 0.2/8.0 GB                                  
                                                       
                    │
  │   396.0 GB/s                                       
                                                       
                    │
  │   DLPerf                                           
                                                       
                    │
  │   -                                                
                                                       
                    │
  │   396.0 DLP/$/hr                                   
                                                       
                    │
  │   Network                                          
                                                       
                    │
  │   250 ports                                        
                                                       
                    │
  │   5835.7 Mbps                                      
                                                       
                    │
  │   1542.6 Mbps                                      
                                                       
                    │
  │   CPU                                              
                                                       
                    │
  │   Xeon® E5-2697 v2                                
                                                       
                     │
  │   12.0/48 CPU                                      
                                                       
                    │
  │   54 / 129.0 GB                                    
                                                       
                    │
  │   Disk                                             
                                                       
                    │
  │   CT2000P2SSD8                                     
                                                       
                    │
  │   429.0 MB/s                                       
                                                       
                    │
  │   106.0 / 128.0 GB                                 
                                                       
                    │
  │   Motherboard                                      
                                                       
                    │
  │   0NK70N                                           
                                                       
                    │
  │   PCIE 3.0/16x\
  When I run a comfyui workflow, there is an error:\
  ((main) ) root@C.23019700:/workspace/ComfyUI$ python 
  main.py --listen 127.0.0.1 --disable-auto-launch 
  --port 18188 --enable-cors-header 
  --disable-auto-launch --force-load-workflow 
  --workflow runcomfy-comfyui-web
  [START] Security scan
  [DONE] Security scan
  ## ComfyUI-Manager: installing dependencies done.
  ** ComfyUI startup time: 2025-07-15 08:44:22.354
  ** Platform: Linux
  ** Python version: 3.12.10 (main, Apr  9 2025, 
  08:55:05) [GCC 11.4.0]
  ** Python executable: /venv/main/bin/python
  ** ComfyUI Path: /workspace/ComfyUI
  ** ComfyUI Base Folder Path: /workspace/ComfyUI
  ** User directory: /workspace/ComfyUI/user
  ** ComfyUI-Manager config path: /workspace/ComfyUI/us
  er/default/ComfyUI-Manager/config.ini
  ** Log path: /workspace/ComfyUI/user/comfyui.log
  Package comfyui-frontend-package==1.23.4 found in 
  requirements file
  Installing package: comfyui-frontend-package==1.23.4
  Requirement already satisfied: 
  comfyui-frontend-package==1.23.4 in 
  /venv/main/lib/python3.12/site-packages (1.23.4)

  [notice] A new release of pip is available: 25.0.1 ->
   25.1.1
  [notice] To update, run: pip install --upgrade pip

  Prestartup times for custom nodes:
     3.1 seconds: 
  /workspace/ComfyUI/custom_nodes/ComfyUI-Manager
     4.4 seconds: /workspace/ComfyUI/custom_nodes/Comfy
  UI-RunComfy-Helper

  WARNING: Potential Error in code: Torch already 
  imported, torch should never be imported before this 
  point.
  Checkpoint files will always be loaded safely.
  Traceback (most recent call last):
    File "/workspace/ComfyUI/main.py", line 135, in 
  <module>
      import execution
    File "/workspace/ComfyUI/execution.py", line 14, in
   <module>
      import comfy.model_management
    File 
  "/workspace/ComfyUI/comfy/model_management.py", line 
  221, in <module>
      total_vram = get_total_memory(get_torch_device())
   / (1024 * 1024)
                                    ^^^^^^^^^^^^^^^^^^
    File 
  "/workspace/ComfyUI/comfy/model_management.py", line 
  172, in get_torch_device
      return torch.device(torch.cuda.current_device())
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
    File "/venv/main/lib/python3.12/site-packages/torch
  /cuda/__init__.py", line 971, in current_device
      _lazy_init()
    File "/venv/main/lib/python3.12/site-packages/torch
  /cuda/__init__.py", line 319, in _lazy_init
      torch._C._cuda_init()
  RuntimeError: config[i] == get()->name() INTERNAL 
  ASSERT FAILED at 
  "/pytorch/c10/cuda/CUDAAllocatorConfig.cpp":250, 
  please report a bug to PyTorch. Allocator backend 
  parsed at runtime != allocator backend parsed at load
   time\
  \
  how should I fix it?

---

_**Assistant**_

<think><details><summary>Thought Process</summary>The user is encountering a CUDA runtime error when running ComfyUI. The error message indicates:

```
RuntimeError: config[i] == get()->name() INTERNAL ASSERT FAILED at "/pytorch/c10/cuda/CUDAAllocatorConfig.cpp":250, please report a bug to PyTorch. Allocator backend parsed at runtime != allocator backend parsed at load time
```

This is a PyTorch CUDA memory allocator issue. Looking at the device config they provided, they have:
- 8.0 GB VRAM (currently using 0.2/8.0 GB)
- CUDA 12.4 support
- Linux platform

The error suggests there's a mismatch between the CUDA allocator configuration at runtime vs load time. This is often caused by:

1. Mixed CUDA versions or installations
2. Environment variables affecting CUDA memory allocation
3. PyTorch version compatibility issues with CUDA

Looking at the ComfyUI code in `main.py`, I can see there are some CUDA-related settings and flags. There's also a `cuda_malloc.py` file mentioned and a `--disable-cuda-malloc` flag mentioned in the warning.

Let me first understand the current setup and then provide solutions.

From the main.py code, I can see:
1. There's a `cuda_malloc_warning()` function that checks for CUDA malloc compatibility
2. There's a `--disable-cuda-malloc` argument available
3. The code imports `cuda_malloc` module

The error happens during PyTorch's CUDA initialization, specifically when trying to get the current device. This suggests the issue is at the PyTorch level, not necessarily ComfyUI specific.

Common solutions for this type of error:
1. Use `--disable-cuda-malloc` flag
2. Set specific environment variables
3. Check PyTorch/CUDA compatibility
4. Clean reinstall of PyTorch

Let me look at the code to understand what the disable-cuda-malloc flag does and provide a comprehensive solution.</details></think>

---

Looking at your error, this is a PyTorch CUDA memory allocator issue. The error occurs during CUDA initialization when there's a mismatch between the CUDA allocator configuration at runtime versus load time.

Let me first check what CUDA malloc options are available in your ComfyUI setup:

---

<details>
            <summary>Searched codebase "cuda_malloc" • **1** results</summary>
        
| File |
|------|
| `undefined` |

</details>

---

<think><details><summary>Thought Process</summary>I found the cuda_malloc.py file. Let me examine it to understand what options are available.</details></think>

---

Read file: cuda_malloc.py

---

<think><details><summary>Thought Process</summary>Now I understand the CUDA malloc setup. Let me also check the CLI args to see what flags are available:</details></think>

---

<details>
            <summary>Searched codebase "cli_args" • **1** results</summary>
        
| File |
|------|
| `undefined` |

</details>

---

<think><details><summary>Thought Process</summary>Let me check the CLI args file to see what options are available:</details></think>

---

<details>
            <summary>Grep search for "cuda.*malloc" • **5** files</summary>
        
| File | Line | Match |
|------|------|-------|
| `file:///Users/yuhan/ComfyUI-1/main.py` | L127 | `import cuda_malloc` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L143 | `def cuda_malloc_warning():` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L146 | `cuda_malloc_warning = False` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L147 | `if "cudaMallocAsync" in device_name:` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L148 | `for b in cuda_malloc.blacklist:` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L150 | `cuda_malloc_warning = True` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L151 | `if cuda_malloc_warning:` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L152 | `logging.warning("\nWARNING: this card most likely does not support cuda-malloc, if you get \"CUDA error\" please run ComfyUI with: --disable-cuda-malloc\n")` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L290 | `cuda_malloc_warning()` |
| `file:///Users/yuhan/ComfyUI-1/cuda_malloc.py` | L5 | `#Can't use pytorch to get the GPU names because the cuda malloc has to be set before the first import.` |
| `file:///Users/yuhan/ComfyUI-1/cuda_malloc.py` | L52 | `def cuda_malloc_supported():` |
| `file:///Users/yuhan/ComfyUI-1/cuda_malloc.py` | L65 | `if not args.cuda_malloc:` |
| `file:///Users/yuhan/ComfyUI-1/cuda_malloc.py` | L77 | `args.cuda_malloc = cuda_malloc_supported()` |
| `file:///Users/yuhan/ComfyUI-1/cuda_malloc.py` | L82 | `if args.cuda_malloc and not args.disable_cuda_malloc:` |
| `file:///Users/yuhan/ComfyUI-1/cuda_malloc.py` | L85 | `env_var = "backend:cudaMallocAsync"` |
| `file:///Users/yuhan/ComfyUI-1/cuda_malloc.py` | L87 | `env_var += ",backend:cudaMallocAsync"` |
| `file:///Users/yuhan/ComfyUI-1/comfy/cli_args.py` | L53 | `cm_group.add_argument("--cuda-malloc", action="store_true", help="Enable cudaMallocAsync (enabled by default for torch 2.0 and up).")` |
| `file:///Users/yuhan/ComfyUI-1/comfy/cli_args.py` | L54 | `cm_group.add_argument("--disable-cuda-malloc", action="store_true", help="Disable cudaMallocAsync.")` |
| `file:///Users/yuhan/ComfyUI-1/comfy/text_encoders/qwen25_tokenizer/vocab.json` | Lundefined | `⟪ 344444 characters skipped ⟫tes":24405,"Ġtob":24406,"CLASS":24407,"Ġcuda":24408,"_det":24409,"-post":24410,"ulu":24411,"Translation":24412,"-hand":24413,".year":24414,"ĠMongo":24415,"Ġunclear":24416,".engine":24417,"WEBPACK":24418,"rices":24419,"_ACCESS":24420,"Ġholid` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L127 | `import cuda_malloc` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L143 | `def cuda_malloc_warning():` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L146 | `cuda_malloc_warning = False` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L147 | `if "cudaMallocAsync" in device_name:` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L148 | `for b in cuda_malloc.blacklist:` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L150 | `cuda_malloc_warning = True` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L151 | `if cuda_malloc_warning:` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L152 | `logging.warning("\nWARNING: this card most likely does not support cuda-malloc, if you get \"CUDA error\" please run ComfyUI with: --disable-cuda-malloc\n")` |
| `file:///Users/yuhan/ComfyUI-1/main.py` | L290 | `cuda_malloc_warning()` |

</details>

---

<think><details><summary>Thought Process</summary>Great! Now I can see the issue clearly. The error is a PyTorch CUDA allocator configuration mismatch. Based on the code analysis, here's what's happening:

1. ComfyUI automatically enables `cudaMallocAsync` for PyTorch 2.0+ (which they're using)
2. This sets the `PYTORCH_CUDA_ALLOC_CONF` environment variable to use `backend:cudaMallocAsync`
3. However, this is causing a runtime vs load time mismatch in the CUDA allocator

The error message itself suggests this is a PyTorch internal issue, but there are several solutions I can provide:

1. **Most direct**: Use `--disable-cuda-malloc` flag
2. **Environment variables**: Set specific CUDA allocation settings
3. **PyTorch environment**: Check compatibility and potentially reinstall

Let me provide a comprehensive solution.</details></think>

---

Based on your error and the ComfyUI code analysis, this is a PyTorch CUDA memory allocator issue. Here are several solutions you can try:

## Solution 1: Disable CUDA Malloc (Recommended)

The simplest fix is to add the `--disable-cuda-malloc` flag to your command:

---

```bash
cd /workspace/ComfyUI && python main.py --listen 127.0.0.1 --disable-auto-launch --port 18188 --enable-cors-header --disable-auto-launch --force-load-workflow --workflow runcomfy-comfyui-web --disable-cuda-malloc
```

---

<think><details><summary>Thought Process</summary>The user chose not to run the command directly. Let me provide them with multiple solutions and explanations so they can choose the best approach.</details></think>

---

Here are several solutions to fix this CUDA allocator error:

## Solution 1: Use --disable-cuda-malloc Flag

Add `--disable-cuda-malloc` to your command:

```bash
python main.py --listen 127.0.0.1 --disable-auto-launch --port 18188 --enable-cors-header --disable-auto-launch --force-load-workflow --workflow runcomfy-comfyui-web --disable-cuda-malloc
```

## Solution 2: Set Environment Variables

Set these environment variables before running ComfyUI:

```bash
export PYTORCH_CUDA_ALLOC_CONF=""
export CUDA_LAUNCH_BLOCKING=1
python main.py --listen 127.0.0.1 --disable-auto-launch --port 18188 --enable-cors-header --disable-auto-launch --force-load-workflow --workflow runcomfy-comfyui-web
```

## Solution 3: Force Different CUDA Memory Backend

```bash
export PYTORCH_CUDA_ALLOC_CONF="backend:native"
python main.py --listen 127.0.0.1 --disable-auto-launch --port 18188 --enable-cors-header --disable-auto-launch --force-load-workflow --workflow runcomfy-comfyui-web
```

## Solution 4: Check PyTorch/CUDA Compatibility

Your system has CUDA 12.4, but there might be a version mismatch. You can check:

```bash
python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}')"
```

## Why This Happens

The error occurs because:
1. ComfyUI automatically enables `cudaMallocAsync` for PyTorch 2.0+
2. There's a mismatch between the CUDA allocator configuration at runtime vs. load time
3. This is often caused by environment variables or PyTorch version compatibility issues

## Recommended Approach

**Start with Solution 1** (`--disable-cuda-malloc`) as it's the safest and most direct fix. This disables the async CUDA memory allocator that's causing the conflict.

If you need better memory performance later, you can try the other solutions once the basic functionality is working.

Would you like me to help you implement any of these solutions?

---

